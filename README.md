```md
# ğŸ“„ PDF RAG Chatbot (Django + Ollama + FAISS)

A Retrieval Augmented Generation (RAG) based PDF Chatbot built using **Python, Django, Ollama, FAISS**, and **HuggingFace embeddings**.  
Users can upload PDF documents and ask questions â€” the system retrieves relevant content and generates answers locally using LLM.

This project runs completely **offline** using Ollama.
```
---

## ğŸš€ Features

- Chat with PDF documents
- Local LLM using Ollama (TinyLlama / Phi-3 Mini)
- FAISS Vector Database
- HuggingFace sentence embeddings
- Django Web Interface
- Fully Offline AI System
- Lightweight (8GB RAM compatible)

---

## ğŸ›  Tech Stack

- Python
- Django
- Ollama
- FAISS
- LangChain
- HuggingFace Sentence Transformers
- HTML

---

## ğŸ“ Project Structure

```

pdf_rag/
â”‚
â”œâ”€â”€ data/                 # PDF files
â”œâ”€â”€ vectorstore/         # FAISS index (auto-generated)
â”œâ”€â”€ chatbot/             # Django app
â”œâ”€â”€ ragweb/              # Django project
â”œâ”€â”€ ingest.py            # PDF ingestion script
â”œâ”€â”€ chat.py              # CLI chatbot
â”œâ”€â”€ venv/                # Virtual environment
â””â”€â”€ manage.py

````

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone Repository

```bash
git clone <your-repo-url>
cd pdf_rag
````

---

### 2ï¸âƒ£ Create Virtual Environment

```bash
python -m venv venv
venv\Scripts\activate
```

---

### 3ï¸âƒ£ Install Dependencies

```bash
pip install django langchain langchain-community faiss-cpu pypdf sentence-transformers ollama
```

---

### 4ï¸âƒ£ Install Ollama & Model

Download Ollama from:

[https://ollama.com](https://ollama.com)

Pull lightweight model:

```bash
ollama pull tinyllama
```

---

### 5ï¸âƒ£ Add PDFs

Place PDF files inside:

```
data/
```

---

### 6ï¸âƒ£ Create Vector Database

```bash
python ingest.py
```

---

### 7ï¸âƒ£ Run Django Server

```bash
cd ragweb
python manage.py runserver
```

Open browser:

```
http://127.0.0.1:8000/
```

---

## ğŸ§  How It Works

1. PDFs are loaded and chunked
2. Text is converted to embeddings
3. Stored in FAISS vector DB
4. User query retrieves relevant chunks
5. Ollama LLM generates answer from context

---

## ğŸ’¡ Use Cases

* Study Assistant
* College Notes Chatbot
* Personal Knowledge Base
* Document QA System
* Portfolio AI Project

---
---

## ğŸ“¸ Screenshots
<img src="assets/chat.jpeg" width="400"/>

---

## ğŸ™Œ Author

Name : Singidi Sai Naga Sudheeer

---

## â­ Acknowledgements

* Ollama
* LangChain
* HuggingFace
* Django

---
